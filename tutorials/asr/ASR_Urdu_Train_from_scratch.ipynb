{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msxCiR8epEZu"
   },
   "source": [
    "## Training from scratch\n",
    "\n",
    "To train from scratch, you need to prepare your training data in the right format and specify your models architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For CUDA OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jALgpGLjmaCw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-08-05 22:24:02 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-08-05 22:24:04 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali.AudioToCharDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "# NeMo's \"core\" package\n",
    "import nemo\n",
    "# NeMo's ASR collection - this collections contains complete ASR models and\n",
    "# building blocks (modules) for ASR\n",
    "import nemo.collections.asr as nemo_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Jasper10x5\n",
      "model:\n",
      "  sample_rate: 16000\n",
      "  labels: /home/hood/KK/MediaAnalysis/ASRdatasetDL/ASR_DL/nemo/nemo/tokenizer_wpe_v1024/vocab.txt\n",
      "  train_ds:\n",
      "    manifest_filepath: /home/hood/KK/MediaAnalysis/ASRdatasetDL/ASR_DL/nemo/nemo.json\n",
      "    sample_rate: 16000\n",
      "    labels: /home/hood/KK/MediaAnalysis/ASRdatasetDL/ASR_DL/nemo/nemo/tokenizer_wpe_v1024/vocab.txt\n",
      "    batch_size: 32\n",
      "    trim_silence: true\n",
      "    max_duration: 16.7\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "  validation_ds:\n",
      "    manifest_filepath: /home/hood/KK/MediaAnalysis/ASRdatasetDL/ASR_DL/nemo/nemo.json\n",
      "    sample_rate: 16000\n",
      "    labels: /home/hood/KK/MediaAnalysis/ASRdatasetDL/ASR_DL/nemo/nemo/tokenizer_wpe_v1024/vocab.txt\n",
      "    batch_size: 32\n",
      "    shuffle: false\n",
      "  preprocessor:\n",
      "    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n",
      "    normalize: per_feature\n",
      "    window_size: 0.02\n",
      "    sample_rate: 16000\n",
      "    window_stride: 0.01\n",
      "    window: hann\n",
      "    features: 64\n",
      "    n_fft: 512\n",
      "    frame_splicing: 1\n",
      "    dither: 1.0e-05\n",
      "  spec_augment:\n",
      "    _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n",
      "    rect_freq: 50\n",
      "    rect_masks: 5\n",
      "    rect_time: 120\n",
      "  encoder:\n",
      "    _target_: nemo.collections.asr.modules.ConvASREncoder\n",
      "    feat_in: 64\n",
      "    activation: relu\n",
      "    conv_mask: true\n",
      "    jasper:\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.2\n",
      "      filters: 256\n",
      "      kernel:\n",
      "      - 11\n",
      "      repeat: 1\n",
      "      residual: false\n",
      "      stride:\n",
      "      - 2\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.2\n",
      "      filters: 256\n",
      "      kernel:\n",
      "      - 11\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.2\n",
      "      filters: 256\n",
      "      kernel:\n",
      "      - 11\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.2\n",
      "      filters: 384\n",
      "      kernel:\n",
      "      - 13\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.2\n",
      "      filters: 384\n",
      "      kernel:\n",
      "      - 13\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.2\n",
      "      filters: 512\n",
      "      kernel:\n",
      "      - 17\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.2\n",
      "      filters: 512\n",
      "      kernel:\n",
      "      - 17\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.3\n",
      "      filters: 640\n",
      "      kernel:\n",
      "      - 21\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.3\n",
      "      filters: 640\n",
      "      kernel:\n",
      "      - 21\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.3\n",
      "      filters: 768\n",
      "      kernel:\n",
      "      - 25\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.3\n",
      "      filters: 768\n",
      "      kernel:\n",
      "      - 25\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 2\n",
      "      dropout: 0.4\n",
      "      filters: 896\n",
      "      kernel:\n",
      "      - 29\n",
      "      repeat: 1\n",
      "      residual: false\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.4\n",
      "      filters: 1024\n",
      "      kernel:\n",
      "      - 1\n",
      "      repeat: 1\n",
      "      residual: false\n",
      "      stride:\n",
      "      - 1\n",
      "  decoder:\n",
      "    _target_: nemo.collections.asr.modules.ConvASRDecoder\n",
      "    feat_in: 1024\n",
      "    num_classes: 87\n",
      "    vocabulary: /home/hood/KK/MediaAnalysis/ASRdatasetDL/ASR_DL/nemo/nemo/tokenizer_wpe_v1024/vocab.txt\n",
      "  optim:\n",
      "    name: novograd\n",
      "    lr: 0.01\n",
      "    betas:\n",
      "    - 0.8\n",
      "    - 0.5\n",
      "    weight_decay: 0.001\n",
      "    sched:\n",
      "      name: CosineAnnealing\n",
      "      warmup_steps: null\n",
      "      warmup_ratio: null\n",
      "      min_lr: 0.0\n",
      "      last_epoch: -1\n",
      "trainer:\n",
      "  gpus: 0\n",
      "  max_epochs: 5\n",
      "  max_steps: null\n",
      "  num_nodes: 1\n",
      "  accelerator: ddp\n",
      "  accumulate_grad_batches: 1\n",
      "  checkpoint_callback: false\n",
      "  logger: false\n",
      "  log_every_n_steps: 1\n",
      "  val_check_interval: 1.0\n",
      "exp_manager:\n",
      "  exp_dir: null\n",
      "  name: Jasper10x5\n",
      "  create_tensorboard_logger: true\n",
      "  create_checkpoint_callback: true\n",
      "  create_wandb_logger: false\n",
      "  wandb_logger_kwargs:\n",
      "    name: null\n",
      "    project: null\n",
      "hydra:\n",
      "  run:\n",
      "    dir: .\n",
      "  job_logging:\n",
      "    root:\n",
      "      handlers: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Some utility imports\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "# This line will print the entire config of the Jasper model\n",
    "config_path = f\"/home/hood/KK/MediaAnalysis/Code Repos/kashbah_ncai/NeMo/examples/asr/conf/jasper/jasper_10x5dr.yaml\"\n",
    "config = OmegaConf.load(config_path)\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "config = OmegaConf.create(config)\n",
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manifest_filepath: /home/hood/KK/MediaAnalysis/ASRdatasetDL/ASR_DL/nemo/nemo.json\n",
      "sample_rate: 16000\n",
      "labels: /home/hood/KK/MediaAnalysis/ASRdatasetDL/ASR_DL/nemo/nemo/tokenizer_wpe_v1024/vocab.txt\n",
      "batch_size: 32\n",
      "trim_silence: true\n",
      "max_duration: 16.7\n",
      "shuffle: true\n",
      "is_tarred: false\n",
      "tarred_audio_filepaths: null\n",
      "tarred_shard_strategy: scatter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(config.model.train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"/home/hood/KK/MediaAnalysis/ASRdatasetDL/ASR_DL/nemo/nemo.json\"\n",
    "val_dataset = \"/home/hood/KK/MediaAnalysis/ASRdatasetDL/ASR_DL/nemo/nemo.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model.train_ds.manifest_filepath = train_dataset\n",
    "config.model.validation_ds.manifest_filepath = val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer config - \n",
      "\n",
      "gpus: 0\n",
      "max_epochs: 5\n",
      "max_steps: null\n",
      "num_nodes: 1\n",
      "accelerator: ddp\n",
      "accumulate_grad_batches: 1\n",
      "checkpoint_callback: false\n",
      "logger: false\n",
      "log_every_n_steps: 1\n",
      "val_check_interval: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Trainer config - \\n\")\n",
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets modify some trainer configs for this demo\n",
    "# Checks if we have GPU available and uses it\n",
    "cuda = 1 if torch.cuda.is_available() else 0\n",
    "config.trainer.gpus = cuda\n",
    "\n",
    "# Reduces maximum number of epochs to 5 for quick demonstration\n",
    "config.trainer.max_epochs = 5\n",
    "\n",
    "# Remove distributed training flags\n",
    "config.trainer.accelerator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**config.trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-08-04 18:59:48 exp_manager:219] Experiments will be logged at /home/hood/KK/MediaAnalysis/Code Repos/kashbah_ncai/NeMo/tutorials/asr/nemo_experiments/Jasper10x5/2021-08-04_18-59-48\n",
      "[NeMo I 2021-08-04 18:59:48 exp_manager:568] TensorboardLogger has been set up\n"
     ]
    }
   ],
   "source": [
    "from nemo.utils.exp_manager import exp_manager\n",
    "\n",
    "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hood/KK/MediaAnalysis/Code Repos/kashbah_ncai/NeMo/tutorials/asr/nemo_experiments/Jasper10x5/2021-08-04_18-59-48'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The exp_dir provides a path to the current experiment for easy access\n",
    "exp_dir = str(exp_dir)\n",
    "exp_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-08-04 18:59:49 collections:173] Dataset loaded with 10 files totalling 0.02 hours\n",
      "[NeMo I 2021-08-04 18:59:49 collections:174] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2021-08-04 18:59:49 collections:173] Dataset loaded with 10 files totalling 0.02 hours\n",
      "[NeMo I 2021-08-04 18:59:49 collections:174] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2021-08-04 18:59:49 features:252] PADDING: 16\n",
      "[NeMo I 2021-08-04 18:59:49 features:269] STFT using torch\n"
     ]
    }
   ],
   "source": [
    "## Building the Jasper Model\n",
    "\n",
    "asr_model = nemo_asr.models.EncDecCTCModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-08-04 18:59:55 modelPT:750] Optimizer config = Novograd (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.8, 0.5]\n",
      "        eps: 1e-08\n",
      "        grad_averaging: False\n",
      "        lr: 0.01\n",
      "        weight_decay: 0.001\n",
      "    )\n",
      "[NeMo I 2021-08-04 18:59:55 lr_scheduler:621] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f4beb353a00>\" \n",
      "    will be used during training (effective maximum steps = 5) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: null\n",
      "    min_lr: 0.0\n",
      "    last_epoch: -1\n",
      "    max_steps: 5\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type                              | Params\n",
      "------------------------------------------------------------------------\n",
      "0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0     \n",
      "1 | encoder           | ConvASREncoder                    | 332 M \n",
      "2 | decoder           | ConvASRDecoder                    | 90.2 K\n",
      "3 | loss              | CTCLoss                           | 0     \n",
      "4 | spec_augmentation | SpectrogramAugmentation           | 0     \n",
      "5 | _wer              | WER                               | 0     \n",
      "------------------------------------------------------------------------\n",
      "332 M     Trainable params\n",
      "0         Non-trainable params\n",
      "332 M     Total params\n",
      "1,330.771 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-08-04 18:59:55 nemo_logging:349] /home/hood/VirtualEnvironments/Nemo/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2021-08-04 18:59:55 patch_utils:49] torch.stft() signature has been updated for PyTorch 1.7+\n",
      "    Please update PyTorch to remain compatible with later versions of NeMo.\n",
      "[NeMo W 2021-08-04 18:59:55 nemo_logging:349] /home/hood/VirtualEnvironments/Nemo/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "    To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "      return torch.floor_divide(self, other)\n",
      "    \n",
      "[NeMo W 2021-08-04 18:59:55 nemo_logging:349] /home/hood/VirtualEnvironments/Nemo/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8529a340ee0e4555828a3891927d3aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: val_loss reached 1718.45935 (best 1718.45935), saving model to \"/home/hood/KK/MediaAnalysis/Code Repos/kashbah_ncai/NeMo/tutorials/asr/nemo_experiments/Jasper10x5/2021-08-04_18-59-48/checkpoints/Jasper10x5--val_loss=1718.46-epoch=0.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: val_loss reached 1607.70251 (best 1607.70251), saving model to \"/home/hood/KK/MediaAnalysis/Code Repos/kashbah_ncai/NeMo/tutorials/asr/nemo_experiments/Jasper10x5/2021-08-04_18-59-48/checkpoints/Jasper10x5--val_loss=1607.70-epoch=1.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 2: val_loss reached 1546.29919 (best 1546.29919), saving model to \"/home/hood/KK/MediaAnalysis/Code Repos/kashbah_ncai/NeMo/tutorials/asr/nemo_experiments/Jasper10x5/2021-08-04_18-59-48/checkpoints/Jasper10x5--val_loss=1546.30-epoch=2.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 3: val_loss reached 1522.49414 (best 1522.49414), saving model to \"/home/hood/KK/MediaAnalysis/Code Repos/kashbah_ncai/NeMo/tutorials/asr/nemo_experiments/Jasper10x5/2021-08-04_18-59-48/checkpoints/Jasper10x5--val_loss=1522.49-epoch=3.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 4: val_loss reached 1542.80298 (best 1522.49414), saving model to \"/home/hood/KK/MediaAnalysis/Code Repos/kashbah_ncai/NeMo/tutorials/asr/nemo_experiments/Jasper10x5/2021-08-04_18-59-48/checkpoints/Jasper10x5--val_loss=1542.80-epoch=4.ckpt\" as top 3\n",
      "Saving latest checkpoint...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(asr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PasvgSEwpWXd"
   },
   "source": [
    "### Specifying Our Model with a YAML Config File\n",
    "\n",
    "We'll build a *Citrinet* model for this tutorial and use *greedy CTC decoder*, using the configuration found in `./configs/citrinet_bpe.yaml`.\n",
    "\n",
    "If we open up this config file, we find model section which describes architecture of our model. A model contains an entry labeled `encoder`, with a field called `jasper` that contains a list with multiple entries. Each of the members in this list specifies one block in our model, and looks something like this:\n",
    "```\n",
    "- filters: 192\n",
    "  repeat: 5\n",
    "  kernel: [11]\n",
    "  stride: [1]\n",
    "  dilation: [1]\n",
    "  dropout: 0.0\n",
    "  residual: false\n",
    "  separable: true\n",
    "  se: true\n",
    "  se_context_size: -1\n",
    "```\n",
    "The first member of the list corresponds to the first block in the QuartzNet/Citrinet architecture diagram. \n",
    "\n",
    "Some entries at the top of the file specify how we will handle training (`train_ds`) and validation (`validation_ds`) data.\n",
    "\n",
    "Using a YAML config such as this helps get a quick and human-readable overview of what your architecture looks like, and allows you to swap out model and run configurations easily without needing to change your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XLUDyWOmo8xZ"
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf, open_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "p1O8JRk1qXX9"
   },
   "outputs": [],
   "source": [
    "params = OmegaConf.load(\"/home/hood/KK/MediaAnalysis/Code Repos/kashbah_ncai/NeMo/examples/asr/conf/jasper/jasper_10x5dr.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHImcuu9qnBl"
   },
   "source": [
    "Let us make the network smaller since `AN4` is a particularly small dataset and does not need the capacity of the general config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "raXzemtIqjL-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Jasper10x5\n",
      "model:\n",
      "  sample_rate: 16000\n",
      "  labels: /home/hood/KK/MediaAnalysis/Code%20Repos/kashbah_ncai/NeMo/vocab/dl_old_train/tokenizer_wpe_v1024/vocab.txt\n",
      "  train_ds:\n",
      "    manifest_filepath: ???\n",
      "    sample_rate: 16000\n",
      "    labels: /home/hood/KK/MediaAnalysis/Code%20Repos/kashbah_ncai/NeMo/vocab/dl_old_train/tokenizer_wpe_v1024/vocab.txt\n",
      "    batch_size: 4\n",
      "    trim_silence: true\n",
      "    max_duration: 50.9\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "  validation_ds:\n",
      "    manifest_filepath: ???\n",
      "    sample_rate: 16000\n",
      "    labels: /home/hood/KK/MediaAnalysis/Code%20Repos/kashbah_ncai/NeMo/vocab/dl_old_train/tokenizer_wpe_v1024/vocab.txt\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "  preprocessor:\n",
      "    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\n",
      "    normalize: per_feature\n",
      "    window_size: 0.02\n",
      "    sample_rate: 16000\n",
      "    window_stride: 0.01\n",
      "    window: hann\n",
      "    features: 64\n",
      "    n_fft: 512\n",
      "    frame_splicing: 1\n",
      "    dither: 1.0e-05\n",
      "  spec_augment:\n",
      "    _target_: nemo.collections.asr.modules.SpectrogramAugmentation\n",
      "    rect_freq: 50\n",
      "    rect_masks: 5\n",
      "    rect_time: 120\n",
      "  encoder:\n",
      "    _target_: nemo.collections.asr.modules.ConvASREncoder\n",
      "    feat_in: 64\n",
      "    activation: relu\n",
      "    conv_mask: true\n",
      "    jasper:\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.2\n",
      "      filters: 256\n",
      "      kernel:\n",
      "      - 11\n",
      "      repeat: 1\n",
      "      residual: false\n",
      "      stride:\n",
      "      - 2\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.2\n",
      "      filters: 256\n",
      "      kernel:\n",
      "      - 11\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.2\n",
      "      filters: 256\n",
      "      kernel:\n",
      "      - 11\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.2\n",
      "      filters: 384\n",
      "      kernel:\n",
      "      - 13\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.2\n",
      "      filters: 384\n",
      "      kernel:\n",
      "      - 13\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.2\n",
      "      filters: 512\n",
      "      kernel:\n",
      "      - 17\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.2\n",
      "      filters: 512\n",
      "      kernel:\n",
      "      - 17\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.3\n",
      "      filters: 640\n",
      "      kernel:\n",
      "      - 21\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.3\n",
      "      filters: 640\n",
      "      kernel:\n",
      "      - 21\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.3\n",
      "      filters: 768\n",
      "      kernel:\n",
      "      - 25\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.3\n",
      "      filters: 768\n",
      "      kernel:\n",
      "      - 25\n",
      "      repeat: 5\n",
      "      residual: true\n",
      "      residual_dense: true\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 2\n",
      "      dropout: 0.4\n",
      "      filters: 896\n",
      "      kernel:\n",
      "      - 29\n",
      "      repeat: 1\n",
      "      residual: false\n",
      "      stride:\n",
      "      - 1\n",
      "    - dilation:\n",
      "      - 1\n",
      "      dropout: 0.4\n",
      "      filters: 1024\n",
      "      kernel:\n",
      "      - 1\n",
      "      repeat: 1\n",
      "      residual: false\n",
      "      stride:\n",
      "      - 1\n",
      "  decoder:\n",
      "    _target_: nemo.collections.asr.modules.ConvASRDecoder\n",
      "    feat_in: 1024\n",
      "    num_classes: 107\n",
      "    vocabulary: /home/hood/KK/MediaAnalysis/Code%20Repos/kashbah_ncai/NeMo/vocab/dl_old_train/tokenizer_wpe_v1024/vocab.txt\n",
      "  optim:\n",
      "    name: novograd\n",
      "    lr: 0.01\n",
      "    betas:\n",
      "    - 0.8\n",
      "    - 0.5\n",
      "    weight_decay: 0.001\n",
      "    sched:\n",
      "      name: CosineAnnealing\n",
      "      warmup_steps: null\n",
      "      warmup_ratio: null\n",
      "      min_lr: 0.0\n",
      "      last_epoch: -1\n",
      "trainer:\n",
      "  gpus: 0\n",
      "  max_epochs: 5\n",
      "  max_steps: null\n",
      "  num_nodes: 1\n",
      "  accelerator: ddp\n",
      "  accumulate_grad_batches: 1\n",
      "  checkpoint_callback: false\n",
      "  logger: false\n",
      "  log_every_n_steps: 1\n",
      "  val_check_interval: 1.0\n",
      "exp_manager:\n",
      "  exp_dir: null\n",
      "  name: Jasper10x5\n",
      "  create_tensorboard_logger: true\n",
      "  create_checkpoint_callback: true\n",
      "  create_wandb_logger: false\n",
      "  wandb_logger_kwargs:\n",
      "    name: null\n",
      "    project: null\n",
      "hydra:\n",
      "  run:\n",
      "    dir: .\n",
      "  job_logging:\n",
      "    root:\n",
      "      handlers: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceelkfIHrHTR"
   },
   "source": [
    "### Training with PyTorch Lightning\n",
    "\n",
    "NeMo models and modules can be used in any PyTorch code where torch.nn.Module is expected.\n",
    "\n",
    "However, NeMo's models are based on [PytorchLightning's](https://github.com/PyTorchLightning/pytorch-lightning) LightningModule and we recommend you use PytorchLightning for training and fine-tuning as it makes using mixed precision and distributed training very easy. So to start, let's create Trainer instance for training on GPU for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3rslHEKeq9qy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=10, amp_level='O1', precision=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLbXg1swre_M"
   },
   "source": [
    "Next, we instantiate and ASR model based on our ``citrinet_bpe.yaml`` file from the previous section.\n",
    "Note that this is a stage during which we also tell the model where our training and validation manifests are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "v7RnwRpprb2S"
   },
   "outputs": [],
   "source": [
    "# Update paths to dataset\n",
    "params.model.train_ds.manifest_filepath = \"/home/hood/KK/MediaAnalysis/ASRdatasetDL/ASR_DL/dl_old_train_resample/dl_old_train_resample.json\"\n",
    "params.model.validation_ds.manifest_filepath = \"/home/hood/KK/MediaAnalysis/ASRdatasetDL/ASR_DL/dl_old_train_resample/dl_old_train_resample.json\"\n",
    "\n",
    "# remove spec augment for this dataset\n",
    "params.model.spec_augment.rect_masks = 0\n",
    "\n",
    "params.model.train_ds.batch_size = 4\n",
    "params.model.validation_ds.batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YVNc9IxdwXp7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-08-05 22:25:16 collections:173] Dataset loaded with 78133 files totalling 268.27 hours\n",
      "[NeMo I 2021-08-05 22:25:16 collections:174] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2021-08-05 22:25:19 collections:173] Dataset loaded with 78133 files totalling 268.27 hours\n",
      "[NeMo I 2021-08-05 22:25:19 collections:174] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2021-08-05 22:25:19 features:252] PADDING: 16\n",
      "[NeMo I 2021-08-05 22:25:19 features:269] STFT using torch\n"
     ]
    }
   ],
   "source": [
    "first_asr_model = nemo_asr.models.EncDecCTCModel(cfg=params.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKZuX5Wavocr"
   },
   "source": [
    "With that, we can start training with just one line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_iFfkFBTryQn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-08-05 22:25:25 modelPT:750] Optimizer config = Novograd (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.8, 0.5]\n",
      "        eps: 1e-08\n",
      "        grad_averaging: False\n",
      "        lr: 0.01\n",
      "        weight_decay: 0.001\n",
      "    )\n",
      "[NeMo I 2021-08-05 22:25:25 lr_scheduler:621] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f615a9ef940>\" \n",
      "    will be used during training (effective maximum steps = 195340) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: null\n",
      "    min_lr: 0.0\n",
      "    last_epoch: -1\n",
      "    max_steps: 195340\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type                              | Params\n",
      "------------------------------------------------------------------------\n",
      "0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0     \n",
      "1 | encoder           | ConvASREncoder                    | 332 M \n",
      "2 | decoder           | ConvASRDecoder                    | 110 K \n",
      "3 | loss              | CTCLoss                           | 0     \n",
      "4 | spec_augmentation | SpectrogramAugmentation           | 0     \n",
      "5 | _wer              | WER                               | 0     \n",
      "------------------------------------------------------------------------\n",
      "332 M     Trainable params\n",
      "0         Non-trainable params\n",
      "332 M     Total params\n",
      "1,330.853 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-08-05 22:25:25 nemo_logging:349] /home/hood/VirtualEnvironments/Nemo/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2021-08-05 22:25:25 patch_utils:49] torch.stft() signature has been updated for PyTorch 1.7+\n",
      "    Please update PyTorch to remain compatible with later versions of NeMo.\n",
      "[NeMo W 2021-08-05 22:25:25 nemo_logging:349] /home/hood/VirtualEnvironments/Nemo/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "    To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "      return torch.floor_divide(self, other)\n",
      "    \n",
      "[NeMo W 2021-08-05 22:25:26 nemo_logging:349] /home/hood/VirtualEnvironments/Nemo/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb6b8b3f16f423a8c2b3dcfc792ff06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-08-06 03:41:04 nemo_logging:349] /home/hood/VirtualEnvironments/Nemo/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:610: LightningDeprecationWarning: Relying on `self.log('val_loss', ...)` to set the ModelCheckpoint monitor is deprecated in v1.2 and will be removed in v1.4. Please, create your own `mc = ModelCheckpoint(monitor='your_monitor')` and use it as `Trainer(callbacks=[mc])`.\n",
      "      warning_cache.deprecation(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-08-06 19:27:30 nemo_logging:349] /home/hood/VirtualEnvironments/Nemo/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:897: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "      rank_zero_warn('Detected KeyboardInterrupt, attempting graceful shutdown...')\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Start training!!!\n",
    "trainer.fit(first_asr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |    8790 MB |    8790 MB |   35681 MB |   26890 MB |\\n|       from large pool |    8715 MB |    8715 MB |   35597 MB |   26882 MB |\\n|       from small pool |      75 MB |      75 MB |      83 MB |       8 MB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |    8790 MB |    8790 MB |   35681 MB |   26890 MB |\\n|       from large pool |    8715 MB |    8715 MB |   35597 MB |   26882 MB |\\n|       from small pool |      75 MB |      75 MB |      83 MB |       8 MB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |    9312 MB |    9322 MB |   10992 MB |    1680 MB |\\n|       from large pool |    9234 MB |    9244 MB |   10914 MB |    1680 MB |\\n|       from small pool |      78 MB |      78 MB |      78 MB |       0 MB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  533789 KB |  540659 KB |   13470 MB |   12949 MB |\\n|       from large pool |  531404 KB |  538252 KB |   13410 MB |   12891 MB |\\n|       from small pool |    2385 KB |    4539 KB |      60 MB |      57 MB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    2259    |    2259    |    7558    |    5299    |\\n|       from large pool |     472    |     472    |    2407    |    1935    |\\n|       from small pool |    1787    |    1787    |    5151    |    3364    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    2259    |    2259    |    7558    |    5299    |\\n|       from large pool |     472    |     472    |    2407    |    1935    |\\n|       from small pool |    1787    |    1787    |    5151    |    3364    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     374    |     399    |     451    |      77    |\\n|       from large pool |     335    |     360    |     412    |      77    |\\n|       from small pool |      39    |      39    |      39    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |     277    |     277    |    3405    |    3128    |\\n|       from large pool |     263    |     263    |    1526    |    1263    |\\n|       from small pool |      14    |      29    |    1879    |    1865    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQ2aSenF90hs"
   },
   "source": [
    "Save the model easily along with the tokenizer using `save_to`. \n",
    "\n",
    "Later, we use `restore_from` to restore the model, it will also reinitialize the tokenizer !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6idt0dfO9z-S"
   },
   "outputs": [],
   "source": [
    "first_asr_model.save_to(\"jasper_asr_03epoch_model.nemo\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "RpHwCTk1-q4t"
   },
   "source": [
    "!ls -l -- *.nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIupynXOxODi"
   },
   "source": [
    "There we go! We've put together a full training pipeline for the model and trained it for 50 epochs.\n",
    "\n",
    "If you'd like to save this model checkpoint for loading later (e.g. for fine-tuning, or for continuing training), you can simply call `first_asr_model.save_to(<checkpoint_path>)`. Then, to restore your weights, you can rebuild the model using the config (let's say you call it `first_asr_model_continued` this time) and call `first_asr_model_continued.restore_from(<checkpoint_path>)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igxnj51WxdSf"
   },
   "source": [
    "We could improve this model by playing with hyperparameters. We can look at the current hyperparameters with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wLR7PfEzxbO1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'novograd', 'lr': 0.01, 'betas': [0.8, 0.5], 'weight_decay': 0.001, 'sched': {'name': 'CosineAnnealing', 'warmup_steps': None, 'warmup_ratio': None, 'min_lr': 0.0, 'last_epoch': -1}}\n"
     ]
    }
   ],
   "source": [
    "print(params.model.optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wfmZWf-xlNV"
   },
   "source": [
    "### After training and hyper parameter tuning\n",
    "\n",
    "Let's say we wanted to change the learning rate. To do so, we can create a `new_opt` dict and set our desired learning rate, then call `<model>.setup_optimization()` with the new optimization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cH31LyZwxi_p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-08-04 19:17:35 modelPT:750] Optimizer config = Novograd (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.8, 0.5]\n",
      "        eps: 1e-08\n",
      "        grad_averaging: False\n",
      "        lr: 0.1\n",
      "        weight_decay: 0.001\n",
      "    )\n",
      "[NeMo I 2021-08-04 19:17:35 lr_scheduler:621] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f54b43fa430>\" \n",
      "    will be used during training (effective maximum steps = 50) - \n",
      "    Parameters : \n",
      "    (warmup_steps: null\n",
      "    warmup_ratio: null\n",
      "    min_lr: 0.0\n",
      "    last_epoch: -1\n",
      "    max_steps: 50\n",
      "    )\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "new_opt = copy.deepcopy(params.model.optim)\n",
    "new_opt.lr = 0.1\n",
    "first_asr_model.setup_optimization(optim_config=new_opt);\n",
    "# And then you can invoke trainer.fit(first_asr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azH7U-K8x0rd"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Let's have a quick look at how one could run inference with NeMo's ASR model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfcfBxIkznU8"
   },
   "source": [
    "Below is an example of a simple inference loop in pure PyTorch. It also shows how one can compute Word Error Rate (WER) metric between predictions and references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Eo2TcBkozlEG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-08-04 19:17:55 collections:173] Dataset loaded with 10 files totalling 0.02 hours\n",
      "[NeMo I 2021-08-04 19:17:55 collections:174] 0 files were filtered totalling 0.00 hours\n",
      "WER = 4.5\n"
     ]
    }
   ],
   "source": [
    "# Bigger batch-size = bigger throughput\n",
    "params['model']['validation_ds']['batch_size'] = 16\n",
    "\n",
    "# Setup the test data loader and make sure the model is on GPU\n",
    "first_asr_model.setup_test_data(test_data_config=params['model']['validation_ds'])\n",
    "first_asr_model.cuda()\n",
    "first_asr_model.eval()\n",
    "\n",
    "# We remove some preprocessing artifacts which benefit training\n",
    "first_asr_model.preprocessor.featurizer.pad_to = 0\n",
    "first_asr_model.preprocessor.featurizer.dither = 0.0\n",
    "\n",
    "# We will be computing Word Error Rate (WER) metric between our hypothesis and predictions.\n",
    "# WER is computed as numerator/denominator.\n",
    "# We'll gather all the test batches' numerators and denominators.\n",
    "wer_nums = []\n",
    "wer_denoms = []\n",
    "\n",
    "# Loop over all test batches.\n",
    "# Iterating over the model's `test_dataloader` will give us:\n",
    "# (audio_signal, audio_signal_length, transcript_tokens, transcript_length)\n",
    "# See the AudioToCharDataset for more details.\n",
    "for test_batch in first_asr_model.test_dataloader():\n",
    "        test_batch = [x.cuda() for x in test_batch]\n",
    "        targets = test_batch[2]\n",
    "        targets_lengths = test_batch[3]        \n",
    "        log_probs, encoded_len, greedy_predictions = first_asr_model(\n",
    "            input_signal=test_batch[0], input_signal_length=test_batch[1]\n",
    "        )\n",
    "        # Notice the model has a helper object to compute WER\n",
    "        first_asr_model._wer.update(greedy_predictions, targets, targets_lengths)\n",
    "        _, wer_num, wer_denom = first_asr_model._wer.compute()\n",
    "        wer_nums.append(wer_num.detach().cpu().numpy())\n",
    "        wer_denoms.append(wer_denom.detach().cpu().numpy())\n",
    "\n",
    "# We need to sum all numerators and denominators first. Then divide.\n",
    "print(f\"WER = {sum(wer_nums)/sum(wer_denoms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1po9EnY28RM"
   },
   "source": [
    "This WER is not particularly impressive and could be significantly improved. You could train longer (try 100 epochs) to get a better number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E35VBsbf4yWy"
   },
   "source": [
    "## Model Improvements\n",
    "\n",
    "You already have all you need to create your own ASR model in NeMo, but there are a few more tricks that you can employ if you so desire. In this section, we'll briefly cover a few possibilities for improving an ASR model.\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "There exist several ASR data augmentation methods that can increase the size of our training set.\n",
    "\n",
    "For example, we can perform augmentation on the spectrograms by zeroing out specific frequency segments (\"frequency masking\") or time segments (\"time masking\") as described by [SpecAugment](https://arxiv.org/abs/1904.08779), or zero out rectangles on the spectrogram as in [Cutout](https://arxiv.org/pdf/1708.04552.pdf). In NeMo, we can do all three of these by simply adding a `SpectrogramAugmentation` neural module. (As of now, it does not perform the time warping from the SpecAugment paper.)\n",
    "\n",
    "Our toy model disables spectrogram augmentation, because it is not significantly beneficial for the short demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMi6Bauy4Jhg"
   },
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(first_asr_model._cfg['spec_augment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATpO9JDw5MzF"
   },
   "source": [
    "If you want to enable SpecAugment in your model, make sure your .yaml config file contains 'model/spec_augment' section which looks like the one above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDTC4fXZ5QnT"
   },
   "source": [
    "### Transfer learning\n",
    "\n",
    "Transfer learning is an important machine learning technique that uses a model’s knowledge of one task to perform better on another. Fine-tuning is one of the techniques to perform transfer learning. It is an essential part of the recipe for many state-of-the-art results where a base model is first pretrained on a task with abundant training data and then fine-tuned on different tasks of interest where the training data is less abundant or even scarce.\n",
    "\n",
    "In ASR you might want to do fine-tuning in multiple scenarios, for example, when you want to improve your model's performance on a particular domain (medical, financial, etc.) or accented speech. You can even transfer learn from one language to another! Check out [this paper](https://arxiv.org/abs/2005.04290) for examples.\n",
    "\n",
    "Transfer learning with NeMo is simple. Let's demonstrate how we could fine-tune the model we trained earlier on AN4 data. (NOTE: this is a toy example). And, while we are at it, we will change the model's vocabulary to demonstrate how it's done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IN0LbDbY5YR1"
   },
   "source": [
    "-----\n",
    "First, let's create another tokenizer - perhaps using a larger vocabulary size than the small tokenizer we created earlier. Also we swap out `sentencepiece` for `BERT Word Piece` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFENXcXw48fc"
   },
   "outputs": [],
   "source": [
    "!python ./scripts/process_asr_text_tokenizer.py \\\n",
    "  --manifest=\"{data_dir}/an4/train_manifest.json\" \\\n",
    "  --data_root=\"{data_dir}/tokenizers/an4/\" \\\n",
    "  --vocab_size=64 \\\n",
    "  --tokenizer=\"wpe\" \\\n",
    "  --no_lower_case \\\n",
    "  --log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5XTyk3M_o7O"
   },
   "source": [
    "Now let's load the previously trained model so that we can fine tune it-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QtyAB9fQ_qbj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-08-06 19:35:11 modelPT:138] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /home/hood/KK/MediaAnalysis/ASRdatasetDL/ASR_DL/dl_old_train_resample/dl_old_train_resample.json\n",
      "    sample_rate: 16000\n",
      "    labels: /home/hood/KK/MediaAnalysis/Code%20Repos/kashbah_ncai/NeMo/vocab/dl_old_train/tokenizer_wpe_v1024/vocab.txt\n",
      "    batch_size: 4\n",
      "    trim_silence: true\n",
      "    max_duration: 50.9\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    \n",
      "[NeMo W 2021-08-06 19:35:11 modelPT:145] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /home/hood/KK/MediaAnalysis/ASRdatasetDL/ASR_DL/dl_old_train_resample/dl_old_train_resample.json\n",
      "    sample_rate: 16000\n",
      "    labels: /home/hood/KK/MediaAnalysis/Code%20Repos/kashbah_ncai/NeMo/vocab/dl_old_train/tokenizer_wpe_v1024/vocab.txt\n",
      "    batch_size: 4\n",
      "    shuffle: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-08-06 19:35:11 features:252] PADDING: 16\n",
      "[NeMo I 2021-08-06 19:35:11 features:269] STFT using torch\n",
      "[NeMo I 2021-08-06 19:35:15 modelPT:438] Model EncDecCTCModel was successfully restored from ./jasper_asr_03epoch_model.nemo.\n"
     ]
    }
   ],
   "source": [
    "restored_model = nemo_asr.models.EncDecCTCModel.restore_from(\"./jasper_asr_03epoch_model.nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "new_opt = copy.deepcopy(params.model.optim)\n",
    "new_opt.lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BBtk30g5sHJ"
   },
   "source": [
    "Now let's update the vocabulary in this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZ3sf2P26SiA"
   },
   "source": [
    "After this, our decoder has completely changed, but our encoder (where most of the weights are) remained intact. Let's fine tune-this model for 20 epochs on AN4 dataset. We will also use the smaller learning rate from ``new_opt` (see the \"After Training\" section)`.\n",
    "\n",
    "**Note**: For this demonstration, we will also freeze the encoder to speed up finetuning (since both tokenizers are built on the same train set), but in general it should not be done for proper training on a new language (or on a different corpus than the original train corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ey9CUkJ5o56"
   },
   "outputs": [],
   "source": [
    "# Check what kind of vocabulary/alphabet the model has right now\n",
    "print(restored_model.decoder.vocabulary)\n",
    "\n",
    "# Lets change the tokenizer vocabulary by passing the path to the new directory,\n",
    "# and also change the type\n",
    "restored_model.change_vocabulary(\n",
    "    new_tokenizer_dir=data_dir + \"/tokenizers/an4/tokenizer_wpe_v64/\",\n",
    "    new_tokenizer_type=\"wpe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7m_CRtH46BjO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-08-06 19:36:00 modelPT:642] Trainer wasn't specified in model constructor. Make sure that you really wanted it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-08-06 19:36:00 modelPT:750] Optimizer config = Novograd (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.8, 0.5]\n",
      "        eps: 1e-08\n",
      "        grad_averaging: False\n",
      "        lr: 0.1\n",
      "        weight_decay: 0.001\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-08-06 19:36:00 lr_scheduler:604] Neither `max_steps` nor `iters_per_batch` were provided to `optim.sched`, cannot compute effective `max_steps` !\n",
      "    Scheduler will not be instantiated !\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-08-06 19:36:03 collections:173] Dataset loaded with 78133 files totalling 268.27 hours\n",
      "[NeMo I 2021-08-06 19:36:03 collections:174] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2021-08-06 19:36:06 collections:173] Dataset loaded with 78133 files totalling 268.27 hours\n",
      "[NeMo I 2021-08-06 19:36:06 collections:174] 0 files were filtered totalling 0.00 hours\n"
     ]
    }
   ],
   "source": [
    "# Use the smaller learning rate we set before\n",
    "restored_model.setup_optimization(optim_config=new_opt)\n",
    "\n",
    "# Point to the data we'll use for fine-tuning as the training set\n",
    "restored_model.setup_training_data(train_data_config=params['model']['train_ds'])\n",
    "\n",
    "# Point to the new validation data for fine-tuning\n",
    "restored_model.setup_validation_data(val_data_config=params['model']['validation_ds'])\n",
    "\n",
    "# Freeze the encoder layers (should not be done for finetuning, only done for demo)\n",
    "# restored_model.encoder.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCmUWZLD63d9"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fs2aK7xB6pAd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2021-08-06 19:43:53 modelPT:642] Trainer wasn't specified in model constructor. Make sure that you really wanted it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-08-06 19:43:53 modelPT:750] Optimizer config = Novograd (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.8, 0.5]\n",
      "        eps: 1e-08\n",
      "        grad_averaging: False\n",
      "        lr: 0.1\n",
      "        weight_decay: 0.001\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-08-06 19:43:53 lr_scheduler:604] Neither `max_steps` nor `iters_per_batch` were provided to `optim.sched`, cannot compute effective `max_steps` !\n",
      "    Scheduler will not be instantiated !\n",
      "\n",
      "  | Name              | Type                              | Params\n",
      "------------------------------------------------------------------------\n",
      "0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0     \n",
      "1 | encoder           | ConvASREncoder                    | 332 M \n",
      "2 | decoder           | ConvASRDecoder                    | 110 K \n",
      "3 | loss              | CTCLoss                           | 0     \n",
      "4 | spec_augmentation | SpectrogramAugmentation           | 0     \n",
      "5 | _wer              | WER                               | 0     \n",
      "------------------------------------------------------------------------\n",
      "332 M     Trainable params\n",
      "0         Non-trainable params\n",
      "332 M     Total params\n",
      "1,330.853 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f05724132d34cb38a3158fc17bd0104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And now we can create a PyTorch Lightning trainer and call `fit` again.\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=1, amp_level='O1', precision=16)\n",
    "trainer.fit(restored_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a33WIRR9B_gR"
   },
   "source": [
    "So we get fast convergence even though the decoder vocabulary is double the size and we freeze the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alykABQ3CNpf"
   },
   "source": [
    "### Fast Training\n",
    "\n",
    "Last but not least, we could simply speed up training our model! If you have the resources, you can speed up training by splitting the workload across multiple GPUs. Otherwise (or in addition), there's always mixed precision training, which allows you to increase your batch size.\n",
    "\n",
    "You can use [PyTorch Lightning's Trainer object](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html?highlight=Trainer) to handle mixed-precision and distributed training for you. Below are some examples of flags you would pass to the `Trainer` to use these features:\n",
    "\n",
    "```python\n",
    "# Mixed precision:\n",
    "trainer = pl.Trainer(amp_level='O1', precision=16)\n",
    "\n",
    "# Trainer with a distributed backend:\n",
    "trainer = pl.Trainer(gpus=2, num_nodes=2, accelerator='ddp')\n",
    "\n",
    "# Of course, you can combine these flags as well.\n",
    "```\n",
    "\n",
    "Finally, have a look at [example scripts in NeMo repository](https://github.com/NVIDIA/NeMo/blob/main/examples/asr/speech_to_text_bpe.py) which can handle mixed precision and distributed training using command-line arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uQGWtRJDF0O"
   },
   "source": [
    "## Under the Hood\n",
    "\n",
    "NeMo is open-source and we do all our model development in the open, so you can inspect our code if you wish.\n",
    "\n",
    "In particular, ``nemo_asr.model.EncDecCTCModelBPE`` is an encoder-decoder model which is constructed using several ``Neural Modules`` taken from ``nemo_asr.modules.`` Here is what its forward pass looks like:\n",
    "```python\n",
    "def forward(self, input_signal, input_signal_length):\n",
    "    processed_signal, processed_signal_len = self.preprocessor(\n",
    "        input_signal=input_signal, length=input_signal_length,\n",
    "    )\n",
    "    # Spec augment is not applied during evaluation/testing\n",
    "    if self.spec_augmentation is not None and self.training:\n",
    "        processed_signal = self.spec_augmentation(input_spec=processed_signal)\n",
    "    encoded, encoded_len = self.encoder(audio_signal=processed_signal, length=processed_signal_len)\n",
    "    log_probs = self.decoder(encoder_output=encoded)\n",
    "    greedy_predictions = log_probs.argmax(dim=-1, keepdim=False)\n",
    "    return log_probs, encoded_len, greedy_predictions\n",
    "```\n",
    "Here:\n",
    "\n",
    "* ``self.preprocessor`` is an instance of ``nemo_asr.modules.AudioToMelSpectrogramPreprocessor``, which is a neural module that takes audio signal and converts it into a Mel-Spectrogram\n",
    "* ``self.spec_augmentation`` - is a neural module of type ```nemo_asr.modules.SpectrogramAugmentation``, which implements data augmentation. \n",
    "* ``self.encoder`` - is a convolutional Jasper, QuartzNet or Citrinet-like encoder of type ``nemo_asr.modules.ConvASREncoder``\n",
    "* ``self.decoder`` - is a ``nemo_asr.modules.ConvASRDecoder`` which simply projects into the target alphabet (vocabulary).\n",
    "\n",
    "Also, ``EncDecCTCModelBPE`` uses the audio dataset class ``nemo_asr.data.AudioToBPEDataset`` and CTC loss implemented in ``nemo_asr.losses.CTCLoss``.\n",
    "\n",
    "You can use these and other neural modules (or create new ones yourself!) to construct new ASR models."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08_ASR_with_Subword_Tokenization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "NeMo",
   "language": "python",
   "name": "nemo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
